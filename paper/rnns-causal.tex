% Set document class
\documentclass[hidelinks,12pt]{article}

% Define packages
\usepackage{hyperref, url} 
\usepackage{graphicx,amsfonts,psfrag,layout,subcaption,array,longtable,lscape,booktabs,dcolumn,amsmath,amssymb,amssymb,amsthm,setspace,epigraph,chronology,color,colortbl,wasysym,diagbox,natbib,colortbl,authblk,commath,upgreek}
\usepackage[]{graphicx}\usepackage[]{color}
\usepackage[page]{appendix}
\usepackage[section]{placeins}
\usepackage[linewidth=1pt]{mdframed}
\usepackage[margin={1in}]{geometry} %1 inch margins

% Telephone code
\def\Plus{\texttt{+}}
\def\Minus{\texttt{-}}

\title{RNN-Based Counterfactual Prediction} 
\author[ ]{Jason Poulos\thanks{PhD Candidate, Department of Political Science, 210 Barrows Hall \#1950, Berkeley, CA 94720-1950. \emph{Email:} \href{mailto:poulos@berkeley.edu}{\nolinkurl{poulos@berkeley.edu}}. \emph{Telephone:} \Plus 1\Minus 510\Minus 642\Minus 6323. I acknowledge support of the National Science Foundation Graduate Research Fellowship (DGE 1106400). This work used the computer resources of Stampede2 at the Texas Advanced Computing Center (TACC) under an Extreme Science and Engineering Discovery Environment (XSEDE) startup allocation (TG-SES180010). The Titan Xp GPU used for this research was donated by the NVIDIA Corporation.}}
\affil[ ]{University of California, Berkeley}
\date{}
\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}

\usepackage[autosize]{dot2texi}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

%% Centered and capitalized section headings
%
%\usepackage{sectsty}
%\sectionfont{\centering\normalfont\scshape}

% Reference labels in the online appendix
\usepackage{xr}
\externaldocument{rnns-causal-sm}

% Footnotes stick at the bottom
\usepackage[bottom]{footmisc}

% Caption keys
\usepackage{mathtools,tikz,caption}
\captionsetup{labelfont=sc,labelsep=period}
\DeclareRobustCommand\sampleline[1]{%
	\tikz\draw[#1] (0,0) (0,\the\dimexpr\fontdimen22\textfont2\relax)
	-- (2em,\the\dimexpr\fontdimen22\textfont2\relax);%
}

\usetikzlibrary{plotmarks}

% Multicolumns for references
\usepackage{multicol}

% Wes Anderson colors

\usepackage{xcolor}

\definecolor{Darjeeling11}{HTML}{FF0000}
\definecolor{Darjeeling15}{HTML}{5BBCD6}

% New footnote characters
\usepackage{footmisc}
\DefineFNsymbols{mySymbols}{{\ensuremath\dagger}{\ensuremath\ddagger}\S\P
	*{**}{\ensuremath{\dagger\dagger}}{\ensuremath{\ddagger\ddagger}}}
\setfnsymbol{mySymbols}

%Rm thanks dagger
\renewcommand\footnotemark{}
%\renewcommand\footnoterule{}

% New tabular environment
\usepackage{tabularx}
\newcolumntype{Y}{>{\raggedleft\arraybackslash}X}% raggedleft column X

% Define appendix 
\renewcommand*\appendixpagename{Appendix}
\renewcommand*\appendixtocname{Appendix}

% Position floats
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}
\renewcommand{\floatpagefraction}{0.35}
\setcounter{totalnumber}{5}

% Colors for highlighting tables
\definecolor{Gray}{gray}{0.9}

% Different font in captions
\newcommand{\captionfonts}{\normalsize}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
	\vskip\abovecaptionskip
	\sbox\@tempboxa{{\captionfonts #1: #2}}%
	\ifdim \wd\@tempboxa >\hsize
	{\captionfonts #1: #2\par}
	\else
	\hbox to\hsize{\hfil\box\@tempboxa\hfil}%
	\fi
	\vskip\belowcaptionskip}
%\makeatother   % Cancel the effect of \makeatletter

% Set Spacing
\doublespacing

% Number assumptions
\newtheorem*{assumption*}{\assumptionnumber}
\providecommand{\assumptionnumber}{}
\makeatletter
\newenvironment{assumption}[2]
{%
	\renewcommand{\assumptionnumber}{Assumption #1}%
	\begin{assumption*}%
		\protected@edef\@currentlabel{#1}%
	}
	{%
	\end{assumption*}
}
\makeatother

% Macros
\newcommand{\Adv}{{\boldsymbol{Adv}}}       
\newcommand{\prp}{{\mathrm{prp}}}                  % How to define new commands 
\newcommand{\calK}{{\cal K}}
\newcommand{\outputs}{{\Rightarrow}}                
\newcommand{\getsr}{{\:\stackrel{{\scriptscriptstyle\hspace{0.2em}\$}}{\leftarrow}\:}}
\newcommand{\andthen}{{\::\;\;}}    %  \: \; for thinspace, medspace, thickspace
\newcommand{\Rand}[1]{{\mathrm{Rand}[{#1}]}}       % A command with one argument
\newcommand{\Perm}[1]{{\mathrm{Perm}[{#1}]}}       
\newcommand{\Randd}[2]{{\mathrm{Rand}[{#1},{#2}]}} % and with two arguments
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\plim}{plim}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\possessivecite}[1]{\citeauthor{#1}'s (\citeyear{#1})} 
\DeclareMathOperator*{\argmin}{arg\,min}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document} 
 
\begin{singlespacing}
\maketitle  
\end{singlespacing}
\thispagestyle{empty}

\pagenumbering{roman}% Roman-numbered pages (start from i)
\begin{abstract}  % abstract of 150 to 250 words
\noindent 
This paper proposes an alternative to the synthetic control method (SCM) for estimating the effect of a policy intervention on an outcome over time. Recurrent neural networks (RNNs) are used to predict counterfactual time-series of treated units using only the outcomes of control units as model inputs. The proposed method does not rely on pre-intervention covariates to construct the synthetic control and is consequently less susceptible to $p$-hacking. RNNs are also capable of handling multiple treated units and can learn nonconvex combinations of control units. In placebo tests, RNNs outperform the SCM in predicting the post-intervention time-series of control units, while yielding a comparable proportion of false positives. The RNN-based approach contributes to a new generation of data-driven machine learning techniques such as matrix completion and the Lasso for generating counterfactual predictions.
\\
\begin{singlespace}
	\emph{Keywords:} Counterfactual Prediction; Recurrent Neural Networks; Randomization Inference; Synthetic Controls; Time-Series Cross-Section Data
\end{singlespace}
\end{abstract}

%Move introduction to second page
\pagebreak
\pagenumbering{arabic}% Arabic-numbered pages (start from 1)

\section{Introduction} 

An important problem in the social sciences is estimating the effect of a binary intervention on an outcome over time. When interventions take place at an aggregate level (e.g., a state), researchers make causal inferences by comparing the post-intervention (``post-period'') outcomes of affected (``treated'') units against the outcomes of unaffected  (``control'') units. A common approach to the problem is the synthetic control method (SCM) \citep{abadie2010synthetic}, which predicts the counterfactual outcomes of treated units by finding a convex combination of control units that match the treated units in term of lagged outcomes. Correlations across units are assumed to remain constant stable time. 

The SCM has several limitations. First, the convexity restriction of the synthetic control estimator precludes dynamic, nonlinear interactions between multiple control units. Intuitively, one can expect that the treated unit may exhibit nonlinear or negative correlations with the control units. \citet{ferman2016revisiting} demonstrate that the convexity restriction implies that the SCM estimator may be biased even if selection into treatment is only correlated with time-invariant unobserved covariates. Second, \citet{ferman2018synthetic} demonstrate that the SCM is generally biased if treatment assignment is correlated with unobserved confounders, even when the number of pre-intervention periods grows \citep{ferman2018synthetic}. Moreover, the authors show that while the SCM minimizes imbalance in pre-period outcomes, the likelihood of finding exact balancing weights vanishes as the number of time periods increase, which results in bias. 

While the strength of the SCM lies in its simplicity in setup and implementation, several problems arise from the lack of guidance on how to specify the SCM estimator. The specification of the estimator can produce very different results: \citet{ferman2018cherry} show, for example, how cherry-picking between common SCM specifications can facilitate $p$-hacking.\citet{kaul2015synthetic} show that the common practice of including lagged versions of the outcome variable as model inputs can render all other covariates irrelevant. \citet{klossner2017comparative} demonstrates that the common practice of using cross-validation to select importance weights can yield multiple values and consequently different results. 

This paper proposes an alternative to the SCM that is capable of automatically selecting appropriate control units at each time-step, allows for nonconvex combinations of control units, and does not rely on pre-intervention covariates. The method uses recurrent neural networks (RNNs) to predict a counterfactual time-series of treated units using only control unit outcomes as model inputs. RNNs are a class of neural networks that take advantage of the sequential nature of time-series data by sharing model parameters across multiple time-steps \citep{el1995}. Non-parametric models such as RNNs are useful for prediction problems because we do not have to assume a functional form on the data. In addition, RNNs can learn the most useful nonconvex combination of control unit outcomes at each time-step for generating counterfactual predictions. Relaxing the convexity restriction is useful when the data-generating process underlying the outcome of interest depends nonlinearly on the history of its inputs. RNNs have been shown to outperform various linear models on time-series prediction tasks \citep{cinar2017position}. 

The proposed method builds on a new literature that uses machine learning methods for data-driven synthetic controls, such as matrix completion \citep{athey2017matrix,2019arXiv190308028P}, or two-stage estimators that reduces data dimensionality via L1-regularized regression \citep{doudchenko2016balancing,carvalho2018arco} or matrix factorization \citep{amjad2018robust} prior to regressing the outcomes on the reduced data. These methods are data-driven in the sense that they are capable of finding an appropriate subset of control units for comparison in the absence of domain knowledge or pre-intervention covariates. 

RNNs are end-to-end trainable and very flexible to a given sequential prediction problem. For example, they are capable of sharing learned parameters across time-steps and multiple treated units. while the SCM can be generalized to handle multiple treated units \citep[e.g.,][]{dube2015pooling,xu2017generalized}, the generalized the SCM is not capable of sharing model weights when predicting the outcomes of multiple treated units. Regularization methods such as dropout can easily be incorporated into RNN architectures to prevent overfitting during the training process, which is problematic when the networks learn an overreliance on a few model inputs. Moreover, an attention mechanism can be included in the model in order to discern the contribution of each model input to the predicted counterfactual. 

In the section immediately below, I describe the approach of using RNNs for counterfactual time-series prediction; Section \ref{eval} details the procedure for evaluating the models in terms of predictive accuracy and statistical significance; Section \ref{placebo} presents the results of the placebo tests and discusses when the proposed method is expected to outperform the SCM; Section \ref{conclusion} concludes by discussing the contributions of the paper and offering potential avenues for future research. 

%\setcounter{section}{1}
\section{Counterfactual prediction} \label{prediction}

The proposed method estimates the causal effect of a discrete intervention in observational panel data; i.e., settings in which treatment is not randomly assigned and there exists both pre- and post-period observations of the outcome of interest. Let $\boldsymbol{Y}$ denote a $\text{N} \times \text{T}$ matrix of outcomes for each unit $i =1, \ldots, \text{N}$ at time $t = 1, \ldots, \text{T}$. $\boldsymbol{Y}$ is incomplete because we observe each element $Y_{it}$ for only the control units and the treated units prior to time of initial treatment exposure, $\text{T}_0 < \text{T}$. Let $\mathcal{O}$ denote the set of $(it)$ values that are observed and $\mathcal{M}$ the set of $(it)$ missing values. Let the values of the $\text{N} \times \text{T}$ complete matrix $\boldsymbol{W}$ be $W_{it} =1$ if $(it) \in \mathcal{M}$ and $W_{it} = 0$ if $(it) \in \mathcal{O}$. Note that the process that generates $W_{it}$ is referred to the treatment assignment mechanism in the causal inference literature \citep{imbens2015causal} and the missing data mechanism in missing data analysis \citep{little2014}. The pattern of missing data is assumed to follow from the simultaneous treatment adoption setting, where treated units are exposed to treatment at time $\text{T}_0$ and every subsequent period. 

This setup is motivated by the \citet{neyman1923} potential outcomes framework, where for each $it$ value there exists a pair of potential outcomes, $Y_{it}(1)$ and $Y_{it}(0)$, which represents the response to treated and control regimes, respectively. The observed outcomes are 

\begin{align} 
Y_{it} = \begin{cases}
Y_{it}(0) 	& \mbox{if } W_{it} = 0  \text{ or } t < \text{T}_0 \\
Y_{it}(1) 	& \mbox{if } W_{it} = 1  \text{ and } t \geq \text{T}_0.
\end{cases} 
\end{align} 

The problem of counterfactual prediction is that we cannot directly observe the missing potential outcomes and instead wish to impute the missing values in $\boldsymbol{Y}(0)$ for treated units with $W_{it} =1$.  The potential outcomes framework explicitly assumes unconfoundedness. In an observational setting, this assumption requires $(\boldsymbol{Y}(0), \boldsymbol{Y}(1)) \independent \boldsymbol{W}| \boldsymbol{Y}(\mathcal{O})$, where $\boldsymbol{Y}(\mathcal{O})$ is the observed data. % which can include observed outcomes for units with similar values or observed outcomes in previous periods

The potential outcomes framework also implicitly assumes treatment is well-defined to ensure that each unit has the same number of potential outcomes \citep{imbens2015causal}. It also excludes interference between units, which would undermine the framework by creating more than two potential outcomes per unit, depending on the treatment status of other units \citep{rubin1990}.

\subsection{Relationship to matrix completion and covariate shift}

The proposed approach is similar to the method of matrix completion via nuclear norm minimization (MC-NNM) proposed by \citet{athey2017matrix} to predict counterfactual outcomes. Matrix completion methods attempt to impute missing entries in a low-rank matrix by solving a convex optimization problem via NNM, even when relatively few values are observed in $\boldsymbol{Y}$ \citep{candes2009exact,candes2010matrix}. The estimator recovers a $\text{N} \times \text{T}$ low-rank matrix by minimizing the sum of squared errors via nuclear norm regularized least squares. The estimator reconstructs the matrix by iteratively replacing missing values with those recovered from a singular value decomposition \citep{mazumder2010spectral}. 

\citet{athey2017matrix} note two drawbacks of MC-NNM. First, the errors may be autocorrelated because the estimator does not account for time-series dependencies in the observed data. The estimator estimate patterns row- and column-wise, but treat the data as perfectly synchronized \citep{yoon2018estimating}. In contrast, the RNN-based approach described in Section \ref{RNNs-section} is exploits the temporal component of the data and therefore does not have the problem of autocorrelated errors. 

Second, the MC-NNM estimator penalizes the errors for each observed value equally without regard to the fact that the probability of missingness (i.e, the propensity score), increases with $t$. \citet{athey2017matrix} suggest weighting the loss function by the propensity score, which is similar to the importance weighting scheme proposed by \citet{cortes2008sample} to address the problem of covariate shift, which is a special case of domain adaptation \citep{huang2007correcting,bickel2009discriminative,cortes2010learning}.\footnote{\citet{schnabel2016recommendations} first connected the matrix completion problem with causal inference in observational settings in the context of recommender systems under confounding. \citet{johansson2016learning} formulates the general problem of counterfactual inference as a covariate shift problem.} 

The covariate shift problem occurs when training and test data are drawn from different distributions. For notational ease, define the set of predictors $\boldsymbol{X}$ = $\boldsymbol{Y}(\mathcal{O}), \, \forall\, t < \text{T}_0$.  In both the matrix completion and RNN-based approach to counterfactual prediction, the models are trained on $\boldsymbol{X}$ to predict $\boldsymbol{Y}(\mathcal{O})$ for $t \geq \text{T}_0$. The trained model is used to impute the missing potential outcomes, $\boldsymbol{Y}(\mathcal{M})$. This approach to counterfactual prediction can be connected to recent work in transfer learning \citep{ben2007analysis,2015arXiv150507818G}. The approach assumes similarity between the distributions of pre-period outcomes of treated and control units. An extension of the RNN-based approach would consider weighting the training loss by the propensity score to reduce any discrepancy between these two distributions. 

\subsection{Nonparametric regression}

In its most basic form, counterfactual prediction can be approached by nonparametrically regressing the post-period observed outcomes on the pre-period observed outcomes,

\begin{equation}\label{eq:np}
  \boldsymbol{\hat{Y}} =  \hat{f_0} \left(\boldsymbol{X}\right) + \upepsilon^{(t)},
\end{equation}
\noindent
where the noise variables $\epsilon^{(t)}$ are assumed to be i.i.d. standard normal and independent of the observed data. 

The nonlinear function $\hat{f_0}$ is estimated by minimizing the mean squared error between the observed and predicted outcomes. The estimated causal effect of the intervention is the difference between the observed time-series of the treated units and the counterfactual time-series that would have been observed in the absence of the intervention:

\begin{equation}\label{eq:pointwise}
  \boldsymbol{\hat{\phi}}^{(t)} = \boldsymbol{Y}(\mathcal{M})^{(t)} - \boldsymbol{\hat{Y}}^{(t)} \hspace{10mm} \text{for } t = \text{T}_0, \ldots, \text{T}. 
\end{equation}

This treatment effect is calculated at every post-period time-step and is thus useful for understanding the temporal evolution of the causal effect.

\section{RNNs for counteractual prediction} \label{RNNs-section}

Unlike linear models, RNNs are non-parametric and are structured for time-series data. RNNs consist of an input $\boldsymbol{X} = \boldsymbol{x}^(1), \ldots \boldsymbol{x}^(n_x)$, output $\boldsymbol{Y} = \boldsymbol{x}^(1), \ldots \boldsymbol{x}^(n_y)$, and a hidden state $\boldsymbol{h}^{(t)}$, where $n_x, n_y \leq T$. At each $t$, RNNs input $\boldsymbol{x}^{(t)}$ and pass it to the $\boldsymbol{h}^{(t)}$, which is updated with a function $g^{(t)}$ using the entire history of the sequence \citep{goodfellow2016deep}:
%
\begin{align}
\boldsymbol{h}^{(t)} &= g^{(t)} \left(\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t)} \right) \nonumber \\ 
&= f_2 \left( \boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)} \right), \label{eq:hidden}
\end{align} where $f_1(\cdot)$ is a nonlinear function that operates on all time-steps and input lengths. The updated hidden layer is used to generate a sequence of output values $\boldsymbol{o}^{(t)}$ in the form of log probabilities that correspond to $\boldsymbol{x}^{(t)}$. The loss function computes $\boldsymbol{\hat{y}}^{(t)} = \mathrm{linear} (\boldsymbol{o}^{(t)})$ and compares this value to $\boldsymbol{x}^{(t)}$.

\subsection{Encoder-decoder networks}

A special variant of RNNs that are suitable for handling variable-length sequential data are encoder-decoder networks \citep{cho2014learning}. Encoder-decoder networks are the standard for neural machine translation \citep{bahdanau2014neural,vinyals2014grammar} and are also widely used for predictive tasks, including speech recognition \citep{chorowski2015attention} and time-series forecasting \citep{zhu2017deep}. 

Encoder-decoder networks are trained to estimate the conditional distribution of the output sequence given the past input sequence, e.g., $p (\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t)})$, where the input and output sequence lengths can differ. The encoder RNN reads in $\boldsymbol{x}^{(t)}$ sequentially and the hidden state of the network updates according to Eq. \ref{eq:hidden}. The hidden state of the encoder is a context vector $\boldsymbol{c}$ that summarizes the input sequence, which is copied over to the decoder RNN. The decoder generates a variable-length output sequence by predicting $\boldsymbol{x}^{(t)}$ given the encoder hidden state and the previous element of the output sequence. Thus, the hidden state of the decoder is updated recursively by

\begin{equation}
\boldsymbol{h}^{(t)} = f \left( \boldsymbol{h}^{(t-1)}, \boldsymbol{X}^{(t-1)}, \boldsymbol{c} \right), \label{eq:decoder}
\end{equation} and the conditional probability of the next element of the sequence is 

\begin{equation}
P (\boldsymbol{x}^{(t)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{X}^{(t-1)}, \boldsymbol{c}) =  f \left( \boldsymbol{h}^{(t)}, \boldsymbol{X}^{(t-1)}, \boldsymbol{c} \right).
\end{equation}  Effectively, the decoder learns to generate outputs $\boldsymbol{x}^{(t)}$ given the previous outputs, conditioned on the input sequence. 

\subsection{Variational autoencoder}

The encoder-decoder model described above predicts the next element of a sequence conditioned on the previous element and an evolving hidden state. While this architecture is effective for many sequence-based predictive tasks, the model does not learn a vector representation of the full sequence. The variational autoencoder (VAE) \citep{kingma2013auto} is a generative model that learns a latent variable model for input sequences such that new sequences can be generated from sampling from the latent space. Similar to encoder-decoder networks, the VAE has an encoder that learns a latent representation of the input sequence and a decoder that maps the representation back to the inputs. The VAE architecture differs from encoder-decoder networks in that the VAE doesn't have a final dense layer that compares the decoder outputs to output sequences $\boldsymbol{x}^{(t)}$ (i.e., it is a ``self-supervised'' technique).\footnote{Fig. SM-\ref{encoder-decoder} and Fig. SM-\ref{vae} illustrates the architectures of encoder-decoder networks and the VAE, respectively.} The other difference is that the VAE maps the inputs to a distribution over latent variables. 

The RNN-based VAE proposed by several researchers \citep{fabius2014variational, chung2015recurrent,bowman2015generating} for sequence modeling consists of an encoder RNN that maps $\boldsymbol{x}^{(t)}$ to a distribution over parameters of a latent space $q$. The model then randomly samples $\boldsymbol{z}$ from the latent distribution, $q(\boldsymbol{z} | \boldsymbol{x}^{(t)}) = q (\boldsymbol{z}; f(\boldsymbol{x}^{(t)}; \theta))$, where $f(\cdot)$ is a differentiable function with respect to parameter $\theta$ \citep[pp. 699][]{goodfellow2016deep}.\footnote{In the empirical applications, $f(\cdot)$ take the form of a log-normal distribution.} Finally, a decoder RNN takes the form of a conditional probability model $P (\boldsymbol{x}^{(t)} | \boldsymbol{z})$. The parameters of the model are learned by maximizing the loss function, which takes the difference between the log-likelihood between the decoder outputs and $\boldsymbol{x}^{(t)}$ and the relative entropy between  $q(\boldsymbol{z} | \boldsymbol{x}^{(t)})$ and the model prior $p (\boldsymbol{z})$. The latter component of the loss function acts as regularizer by forcing the learned latent distribution to be similar to the model prior. 

\section{Predictive accuracy and statistical significance} \label{eval}

In applications in which the counterfactual time-series is known (i.e., placebo tests) models can be evaluated in terms of the MSPE between the predicted and actual post-intervention time-series among control units. Specifically, I calculate:

\begin{equation}
	\text{MSPE} = \frac{1}{\text{T}-n} \sum_{n+1: \text{T}}^{\text{T}} \left(\hat{\phi}^{(t)} \right)^2, \label{mspe}
\end{equation} where $\hat{\phi}^{(t)}$ is defined in Eq.~\ref{eq:pointwise}. Eq. \ref{mspe} measures the accuracy of the counterfactual predictions, and consequently the accuracy of the estimated treatment effect. However, this metric does not tell us anything about the statistical significance of estimated treatment effects. 

\citet{abadie2010synthetic} propose a randomization inference approach for calculating the exact distribution of placebo effects under the sharp null hypothesis of no impact. \citet{cavallo2013catastrophic} extends the placebo-based testing approach to the case of multiple (placebo) treated units by constructing a distribution of \emph{average} placebo effects under the null hypothesis. \citet{firpo2018synthetic} derive the conditions under which the randomization inference approach is valid from a finite sample perspective.\footnote{\citet{hahn2017synthetic} analyze the approach from a repeated sampling perspective.} Randomization $p$-values are obtained following these steps:

\begin{enumerate} 
	\item Estimate the observed test static $\mu^{*}$ by estimating Eq. \ref{mspe} for all $J$, which results in a matrix of dimension $(\text{T}-n) \times J$. Taking the row-wise mean results in a $\text{T}-n$-length array of observed average placebo treated effects. 
	\item Calculate every possible average placebo effect $\mu$ by randomly sampling without replacement which $J-1$ control units are assumed to be treated. There are $\mathcal{Q} = \sum\limits_{g=1}^{J-1} {J \choose g}$ possible average placebo effects. The result is a matrix of dimension $(\text{T}-n) \times \mathcal{Q}$. Note that $\mathcal{Q}$ can be computationally burdensome when there are many control units. In the applications described below, I set $\mathcal{Q} = 10,000$ in which $J > 16$.
	\item Take a column-wise sum of the number of $\mu$ that are greater than or equal to $\mu^{*}$.  
\end{enumerate}

Each element of the $(\text{T}-n) \times J$ matrix of counts obtained from the last step is divided by $\mathcal{Q}$ to estimate an array of exact two-sided $p$ values, $\hat{p}$. I then calculate a single false positive rate by $\text{FPR} = \frac{\text{FP}}{(\text{T}-n) \times J}$, where $\text{FP}$ is the number of false positives defined as the number of $p$-values less than or equal to $\alpha=0.5$ and $J$ is the number of placebo treated units.

\subsection{Randomization confidence intervals}

Assuming that treatment has a constant additive effect $\Delta$, I construct an interval estimate for $\Delta$ by inverting the randomization test. Let $\delta_\Delta$ be the test statistic calculated by subtracting all possible $\mu$ by $\Delta$. I derive a two-sided randomization confidence interval by collecting all values of $\delta_\Delta$ that yield $\hat{p}$ values greater than or equal to a significance level $\alpha$. I find the endpoints of the confidence interval by randomly sampling 1,000 values of $\Delta$.

\section{SCM placebo tests} \label{placebo}

I evaluate the proposed RNN-based approach on three datasets common to the SCM literature. In each dataset, I remove the actual treated unit and evaluate the models on their ability to produce low error rates on control units; i.e., estimating treatment effects of zero. A secondary evaluation criteria is the probability of falsely rejecting the null hypothesis of the randomization test as measured the FPR. The synthetic control estimator is implemented using the publicly available \textsf{R} code associated with each of the three referenced studies, and importance weights are chosen by cross-validation. 

\subsection{RNNs implementation details}

In the following application, I train a baseline RNN in the form of a single unidirectional Long Short-Term Memory (LSTM) network \citep{schmidhuber1997long} with output space dimensionality equivalent to the number of treated units. The encoder takes the form of a two-layer bidirectional LSTMs, each with 128 hidden units, and the decoder is a single-layer Gated Recurrent Unit (GRU) \citep{chung2014} also with 128 hidden units. An attention mechanism in the form of a softmax mask is included before the first hidden layer in order to generate a normalized distribution of the importance of each time-step regarding an input. In contrast to attention mechanisms used in neural machine translation to assist networks in learning the correct alignment between image pixels and target characters \citep{cho2014learning,2017arXiv171204046P}, the attention mechanism used in this paper is not expected to help the model perform better, but to help understand which time-steps and inputs contribute to the prediction.

RNN weights are learned with stochastic gradient descent on mean squared prediction error (MSPE), $L^{(t)}_{\text{MSPE}} = \E \left[\left(\boldsymbol{x}^{(t)} - \boldsymbol{\hat{y}}^{(t)} \right)^2 \right]$ using \texttt{Adam} stochastic optimization \citep{kingma2014adam}. As a regularization strategy, I apply dropout to the inputs and L2 regularization losses to the network weights. The networks are implemented with the \texttt{Keras} neural network library \citep{chollet2015keras} in Python on top of a TensorFlow backend. RNNs are trained in batches of size four or eight for 5,000 to 10,000 epochs, which takes about 20 minutes to run on a 12GB NVIDIA Titan Xp GPU.

\subsection{Basque Country} 

%	Samples are constructed using a sliding window with step size one, where each sliding window contains the previous 28 days as input, and aims to forecast the upcoming day. The raw data are log-transformed to alleviate exponential effects. 

\citet{abadie2003economic} estimate the economic impact of terrorism in the Basque Country during the period of 1968 to 1997 by comparing per-capita gross domestic product (GDP) in the Basque Country against a synthetic control region without terrorism. The synthetic control is constructed using pre-period measures of illiteracy, educational attainment, investment, and means of GDP. The time series begins in 1955, which leaves only $n = 14$ pre-period time-steps. 

Figs. SM-\ref{basque-plot-effects} plots estimated treatment effects on control units for each model. We observe considerable variability in the SCM estimates and not as much variability in the RNNs. This is explained by the fact that the SCM cannot handle multiple (placebo) treated units and thus a separate model has to be run for each (placebo) treated unit. RNNs can handle multiple treated units and thus benefit from parameter sharing across treated units. The standard deviation from the mean MPSE, which is reported in the first column of Panel A of Table \ref{fig:benchmark-table}, indicates that the SCM is comparatively more variable in terms of its predictive accuracy. The baseline LSTM yields the lowest mean MSPE, $0.007 \pm 0.002$. 

\begin{table}[htbp]
\begin{center}
\caption{Evaluation metrics on SCM placebo tests.\label{fig:benchmark-table}}
\resizebox{\width}{!}{\input{/media/jason/Dropbox/github/rnns-causal/paper/rnns-results}}
\resizebox{\width}{!}{\input{/media/jason/Dropbox/github/rnns-causal/paper/rnns-fpr}}\\
\footnotesize{{NOTE: Error bars represent $\pm$ one standard deviation from the MSPE.}}
\end{center}
\end{table}

Fig. SM-\ref{basque-plot-pvalues} plots the per-period randomization $p$-values corresponding to treatment effects on treated and control units. $p$-values corresponding to treatment effects on the actual treated unit (i.e., Basque Country) are made by comparing the per-period treatment effects on Basque Country against the null distribution of average placebo effects. The SCM has the comparatively lowest FPR (Panel B of Table \ref{fig:benchmark-table}) among all models: the model falsely rejects the null hypothesis about a quarter of the time. Note that the reported $p$-values are not adjusted for multiple comparisons.

\subsection{California} 

\citet{abadie2010synthetic} applies the SCM to estimate the effects of a large-scale tobacco control program implemented in California in 1988. The study spans the period of 1970 to 2000, providing $n=19$ pre-period time-steps.  The synthetic control is constructed using pre-period covariates including income, beer sales, demographics, and means of the dependent variable, which is log per-capita cigarette consumption. 

Fig. SM-\ref{california-plot-effects} shows that for RNNs, control unit treatment effects are tightly centered around zero --- as expected --- whereas SCM control treatment effects (Fig. SM-\ref{california-plot-effects-synth}) are more dispersed. Encoder-decoder networks yield the lowest mean MSPE, $0.005 \pm 0.004$, while yielding comparatively higher FPR.  

\subsection{West Germany} 

Lastly, \citet{abadie2015comparative} constructs a synthetic West Germany in order to estimate the impact of the 1990 German reunification on log real per-capita GDP during a post-period that extends to 2003. The time-series begins in 1960, which leaves $n=30$ pre-period time-steps. The synthetic control is constructed using pre-period means of GDP, trade, industry, schooling, and investment. 

The SCM and RNN-based approaches both assume the absence of spillover effects. \citet{abadie2015comparative} acknowledge that spillover effects is a valid concern in their study because German reunification likely have effects on GDP in the 16 OECD member countries that serve as controls. Indeed, Fig. SM-\ref{germany-plot-effects} shows that RNNs estimate mostly positive (and increasing) treatment effects on control units, which suggests that reunification might have had a less negative impact on German GDP than the authors' estimates suggest.

Overall, the baseline LSTM yields the lowest error in terms of mean MSPE, $0.02 \pm 0.01$, with a FRP comparable to the SCM.

\subsection{Discussion}

I compare the predictive accuracy of the RNN-based approach against the SCM by running a series of placebo tests using data from three datasets common to the SCM literature. The models are evaluated primarily on their ability to produce low error rates on control units (i.e., estimating treatment effects of zero); a secondary evaluation criteria is minimizing the probability of falsely rejecting the null hypothesis of the randomization test. 

I find that either encoder-decoder networks or LSTM outperform the SCM on each of the three datasets in terms of having the lowest MSPE, with FPRs comparable to the SCM. The baseline LSTM outperforms encoder-decoder networks in two of the three datasets. When applied to datasets with low-dimensional predictor sets, the LSTM performs well but deeper networks such as encoder-decoder networks are susceptible to overfitting. Overfitting in this case means that the networks learn dependencies on a small subset of predictors and cannot generalize well to unseen data. Overfitting occurs when training encoder-decoder networks on the Basque Country dataset (Fig. SM-\ref{encoder-decoder-loss-basque-treated}), which is has the lowest dimensions of the three SCM datasets. Even in this case of obvious overfitting, model check-pointing is employed so that the model with the lowest validation error is used to produce counterfactual time-series.

The results suggest that RNNs should outperform the SCM in all cases as long as the complexity of the network architecture is proportional to the dimension of the predictor set. Encoder-decoder networks outperform the other models when the predictor set is comparatively large (i.e., $J=38$ in the California dataset), while the baseline LSTM outperforms all other models on smaller predictor sets ($J=16$ for Basque Country and West Germany datasets). 

\section{Application: Homestead acts and state capacity} \label{state-capacity}

In this section, I estimate the causal impacts of homestead acts on state government education spending. I create a state-level measure of state government education spending from the records of 48 state governments during the period of 1783 to 1932 \citep{sylla1993sources} and the records of 16 state governments during the period of 1933 to 1937 \citep{sylla1995sourcesa,sylla1995sourcesb}. Comparable measures for 48 states are drawn from U.S. Census special reports for the years 1902, 1913, 1932, 1942, 1962, 1972, and 1982 \citep{haines2010}.

The data pre-processing steps are as follows. The measure is inflation-adjusted according to the U.S. Consumer Price Index \citep{williamson2017seven} and scaled by the total free population in the decennial census \citep{haines2010}. Missing values are imputed separately in the pre- and -post-periods by carrying the last observation forward and remaining missing values are imputed by carrying the next observation backward. The raw outcomes data are log-transformed to alleviate exponential effects. Lastly, I remove states with no variance in the pre-period outcomes, resulting in complete $\text{N} \times \text{T}$ matrices of size $33 \times 159$ and $34 \times 158$ for the expenditures and revenues outcomes, respectively. 

In this application, public land states are the treated units and state land states --- i.e., states that were not crafted from the public domain and were therefore not directly affected by homestead policies --- serve as control units. This group includes states of the original 13 colonies, Maine, Tennessee, Texas, Vermont, and West Virginia. Aggregating to the state level approximately 1.46 million individual land patent records authorized under the HSA,\footnote{Land patent records provide information on the initial transfer of land titles from the federal government and are made accessible online by the U.S. General Land Office (\url{https://glorecords.blm.gov}).} I determine that the earliest homestead entries occurred in 1869 in about half of the western frontier states, about seven years following the enactment of the HSA.

%assume distribution of X_c similar to X_t (might be violated in the application)
%States that were not crafted from the public domain and were therefore not directly affected by the homestead acts --- the states of the original 13 colonies, Maine, Tennessee, Texas, Vermont, and West Virginia --- are not appropriate control units because their governments and markets had already been well-developed by the time of the passage of the homestead laws. Additionally, it is likely that state land states were indirectly affected by the out-migration of homesteaders from frontier states. 

I train a encoder-decoder network to predict the counterfactual time-series of public land states, using only their previous history to generate predictions. Similar to the placebo tests on the SCM datasets, I evaluate the models two ways: first, I monitor the loss over 2,000 training epochs and save the model weights with the lowest error on a validation set consisting of the final 10\% of the time-series.\footnote{The models use both dropout and L2 regularization to control for overfitting on the training set. Figs. SM-\ref{encoder-decoder-loss-capacity-south} and SM-\ref{encoder-decoder-loss-capacity-west} record the training history of each model.} Second, I calculate MSPE (Eq.~\ref{mspe}) on $J=18$ state land states that serve as placebo treated units. Encoder-decoder networks outperform the baseline LSTM in terms of minimizing the MSPE in placebo tests (Tables SM-\ref{encoder-decoder-mpse} and SM-\ref{lstm-mpse}).

Fig.~\ref{state-capacity-plot} plots the observed and counterfactual time-series for each outcome and region. Counterfactual predictions of state government finances in the absence of homestead acts generally tracks the observed time-series until the turn of the century, at which the counterfactual flattens and diverges from the increasing observed time-series. This delay can potentially be explained by the facts that homesteaders were required to make improvements on land for five years before filing a grant and homestead entries did not substantially accumulate until after the 1889 cash-entry restriction (Figs. SM-\ref{homestead-map} and SM-\ref{acres-time}). 

Taking the difference between the observed and predicted time-series (Eq.~\ref{eq:pointwise}) yields time-specific estimates of treatment effects. Fig.~\ref{state-capacity-plot-effects} plots the temporal evolution of treatment effect estimates over the entire post-period and 95\% randomization confidence intervals that are constructed by inverting the randomization test described in the previous section.\footnote{Fig. SM-\ref{state-capacity-plot-pvalues} plots the time-specific estimates of randomization $p$-values inferred from the exact distribution of average placebo effects under the null hypothesis.} Fifty years after its passage, the estimated impact of the HSA on western state government education spending and revenue is 0.005 log points [-0.16, 0.19], and 0.61 log points [-0.19, 1.53], respectively. The confidence intervals surrounding these time-specific estimates contain zero, which implies that the estimated impacts are not significantly more extreme than the placebo treated effects estimated at the same time-step. The confidence interval on the estimated impact of the HSA on western state government expenditure in 1912, an increase of 0.17 log points [0.004, 0.3], does not contains zero, which implies that the estimated impact is significantly more extreme than the placebo treated effects. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{/media/jason/Dropbox/github/land-reform/unsourced-plots/mc-educ-pc.png}
	\caption{MC-NNM estimates of treatment exposure on state government education spending, 1809 to 1982.\label{mc-educ-pc}} 
\end{figure}

An important characteristic of the estimates plotted in Fig.~\ref{state-capacity-plot-effects} is the progressive widening of confidence intervals. Intuitively, counterfactual predictions become more uncertain as we move farther into the future. In the present application, confidence intervals may become implausibly wide because the post-period extends well into the twentieth century. In order to compare with the DD estimates described in the section below, I average over the entire post-period and find no evidence that the HSA impacted western state government education spending, 0.07 [-0.32, 0.46], expenditure, 0.16 [-0.21, 0.57], or revenue, 0.05 [-0.33, 0.46]. Estimates on the impact of the SHA on state capacity in the South are in the same direction and similar magnitudes. 

The attention mechanism embedded within the networks provides clues as to which predictors the networks favor at each time-step during the training process. Fig. 3 plots the attention mechanism applied to the test data, where each cell of the heatmap represents the mean attention weight regarding each time-step and predictor across all test samples. The attention distribution suggests that the networks favor heavily winner margins from the 1975 election and also winner margins from the 1988 and 2003 elections when predicting the post-period. Attention is generally distributed evenly across predictors, which suggests that the networks rely on many different non-treated cities to construct the counterfactual.

\begin{figure}[htbp]
	\centering
%	\includegraphics[width=\textwidth]{/media/jason/Dropbox/github/land-reform/unsourced-plots/mc-educ-pc.png}
	\caption{Attention mechanism as a function of predictors (i.e., winner margins by city) and time-steps for encoder-decoder
		networks. Attention is the normalized (softmax) distribution of the importance of each time-step regarding a predictor.\label{attn-plot}} 
\end{figure}

\subsection{Discussion}

% bandiera2018nation
% states adopted compulsory schooling laws as a nation-building tool to instill civic values to the tens of millions of culturally diverse migrants who arrived during the ‘Age of Mass Migration’ between 1850 and 1914.

%alesina2013nation
% provision of mass primary education as nation-building policy
% means to homogenize the population

%meyer1979public
% we argue that the spread of schooling in the rural North and West can best be understood as a social movement implementing a commonly held ideology of nation-building
% The rapid spread of public schooling across the continent during the 19th century is one of the most dramatic examples of institution-building in American history. 

%Galor et al. [2009] show that state schooling expenditures are significantly correlated to land inequality

\section{Conclusion} \label{conclusion}

This paper makes a methodological contribution in proposing a novel alternative to the SCM for estimating the effect of a policy intervention on an outcome over time in settings where appropriate control units are unavailable. The SCM is growing in popularity in the social sciences despite its limitations --- the most obvious being that the choice of specification can lead to different results, and thus facilitate $p$-hacking. By inputting only control unit outcomes and not relying on pre-period covariates, the proposed method offers a more principled approach than the SCM. 

In placebo tests, RNN-based models outperform the SCM in terms of predictive accuracy while yielding a comparable proportion of false positives. RNNs have advantages over the SCM in that they are structured for sequential data and can learn nonconvex combinations of predictors, which is beneficial when the data-generating process underlying the outcome of interest depends nonlinearly on the history of its inputs. RNNs are also capable of handling multiple treated units and can learn nonconvex combinations of control units, which is useful because the model can share parameters across treated units, and thus generate more precise predictions in settings in which treated units share similar data-generating processes.

The RNN-based approach joins a new generation of data-driven machine learning techniques for generating counterfactual predictions. Machine learning techniques in general have an advantage over the SCM in that they automatically choose appropriate predictors without relying on pretreatment covariates; this capability limits ``researcher degrees of freedom'' that arises from choices on how to specify the model. RNNs have an advantage over alternative machine learning algorithms because they are specifically structured to exploit the sequential nature of time-series data by sharing model parameters across time-steps.

Future research might investigate through simulations how the interaction between RNN complexity (as determined by the number of hidden layers or nodes) and data dimensionality impacts predictive accuracy. Simulations will also allow us to assess the exact impact of data dimensionality, the proportion of treated units, convexity versus non-convexity in the modeled relationship, and the length of the pre-period on the choice between RNNs and the SCM. % Future work might formalize a set of assumptions necessary for the approach to be valid and provide further proof of consistency of the estimator under these assumptions.

\newpage

\bibliographystyle{rss}
\begin{singlespace}
	\begin{footnotesize}
		\begin{multicols}{2}
			\bibliography{references}
		\end{multicols}
	\end{footnotesize}
\end{singlespace}

\itemize
\end{document}