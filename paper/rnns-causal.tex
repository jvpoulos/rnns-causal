% Set document class
\documentclass[hidelinks,12pt]{article}

% Define packages
\usepackage{hyperref, url} 
\usepackage{graphicx,amsfonts,psfrag,layout,subcaption,array,longtable,lscape,booktabs,dcolumn,amsmath,amssymb,amssymb,amsthm,setspace,epigraph,chronology,color,colortbl,wasysym,diagbox,natbib,colortbl,authblk,commath,upgreek}
\usepackage[]{graphicx}\usepackage[]{color}
\usepackage[page]{appendix}
\usepackage[section]{placeins}
\usepackage[linewidth=1pt]{mdframed}
\usepackage[margin={1in}]{geometry} %1 inch margins

% Telephone code
\def\Plus{\texttt{+}}
\def\Minus{\texttt{-}}

\title{RNN-Based Counterfactual Time-Series Prediction} 
\author[ ]{Jason Poulos\thanks{PhD Candidate, Department of Political Science, 210 Barrows Hall \#1950, Berkeley, CA 94720-1950. \emph{Email:} \href{mailto:poulos@berkeley.edu}{\nolinkurl{poulos@berkeley.edu}}. \emph{Telephone:} \Plus 1\Minus 510\Minus 642\Minus 6323. I acknowledge support of the National Science Foundation Graduate Research Fellowship (DGE 1106400). This work used the computer resources of Stampede2 at the Texas Advanced Computing Center (TACC) under an Extreme Science and Engineering Discovery Environment (XSEDE) startup allocation (TG-SES180010). The Titan Xp GPU used for this research was donated by the NVIDIA Corporation.}}
\affil[ ]{University of California, Berkeley}
\date{}
\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}

\usepackage[autosize]{dot2texi}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

%% Centered and capitalized section headings
%
%\usepackage{sectsty}
%\sectionfont{\centering\normalfont\scshape}

% Reference labels in the online appendix
\usepackage{xr}
\externaldocument{rnns-causal-sm}

% Footnotes stick at the bottom
\usepackage[bottom]{footmisc}

% Caption keys
\usepackage{mathtools,tikz,caption}
\captionsetup{labelfont=sc,labelsep=period}
\DeclareRobustCommand\sampleline[1]{%
	\tikz\draw[#1] (0,0) (0,\the\dimexpr\fontdimen22\textfont2\relax)
	-- (2em,\the\dimexpr\fontdimen22\textfont2\relax);%
}

\usetikzlibrary{plotmarks}

% Multicolumns for references
\usepackage{multicol}

% Wes Anderson colors

\usepackage{xcolor}

\definecolor{Darjeeling11}{HTML}{FF0000}
\definecolor{Darjeeling15}{HTML}{5BBCD6}

% New footnote characters
\usepackage{footmisc}
\DefineFNsymbols{mySymbols}{{\ensuremath\dagger}{\ensuremath\ddagger}\S\P
	*{**}{\ensuremath{\dagger\dagger}}{\ensuremath{\ddagger\ddagger}}}
\setfnsymbol{mySymbols}

%Rm thanks dagger
\renewcommand\footnotemark{}
%\renewcommand\footnoterule{}

% New tabular environment
\usepackage{tabularx}
\newcolumntype{Y}{>{\raggedleft\arraybackslash}X}% raggedleft column X

% Define appendix 
\renewcommand*\appendixpagename{Appendix}
\renewcommand*\appendixtocname{Appendix}

% Position floats
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}
\renewcommand{\floatpagefraction}{0.35}
\setcounter{totalnumber}{5}

% Colors for highlighting tables
\definecolor{Gray}{gray}{0.9}

% Different font in captions
\newcommand{\captionfonts}{\normalsize}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
	\vskip\abovecaptionskip
	\sbox\@tempboxa{{\captionfonts #1: #2}}%
	\ifdim \wd\@tempboxa >\hsize
	{\captionfonts #1: #2\par}
	\else
	\hbox to\hsize{\hfil\box\@tempboxa\hfil}%
	\fi
	\vskip\belowcaptionskip}
%\makeatother   % Cancel the effect of \makeatletter

% Set Spacing
\doublespacing

% Macros
\newcommand{\Adv}{{\mathbf{Adv}}}       
\newcommand{\prp}{{\mathrm{prp}}}                  % How to define new commands 
\newcommand{\calK}{{\cal K}}
\newcommand{\outputs}{{\Rightarrow}}                
\newcommand{\getsr}{{\:\stackrel{{\scriptscriptstyle\hspace{0.2em}\$}}{\leftarrow}\:}}
\newcommand{\andthen}{{\::\;\;}}    %  \: \; for thinspace, medspace, thickspace
\newcommand{\Rand}[1]{{\mathrm{Rand}[{#1}]}}       % A command with one argument
\newcommand{\Perm}[1]{{\mathrm{Perm}[{#1}]}}       
\newcommand{\Randd}[2]{{\mathrm{Rand}[{#1},{#2}]}} % and with two arguments
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\plim}{plim}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\possessivecite}[1]{\citeauthor{#1}'s (\citeyear{#1})} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document} 
 
\begin{singlespacing}
\maketitle  
\end{singlespacing}
\thispagestyle{empty}

\pagenumbering{roman}% Roman-numbered pages (start from i)
\begin{abstract}  % abstract of 150 to 250 words
\noindent 
This paper proposes an alternative to the synthetic control method (SCM) for estimating the effect of a policy intervention on an outcome over time. Recurrent neural networks (RNNs) are used to predict counterfactual time-series of treated units using only the outcomes of control units as model inputs. The proposed method does not rely on pre-period covariates to construct the synthetic control and is consequently less susceptible to $p$-hacking. RNNs are also capable of handling multiple treated units and can learn nonconvex combinations of control units. In placebo tests, RNNs outperform SCM in predicting the post-intervention time-series of control units, while yielding a comparable proportion of false positives. The RNN-based approach contributes to a new generation of data-driven machine learning techniques such as matrix completion and the Lasso for generating counterfactual predictions.
\\
\begin{singlespace}
	\emph{Keywords:} Counterfactual Prediction; Recurrent Neural Networks; Randomization Inference; Synthetic Controls; Time-Series Cross-Section Data
\end{singlespace}
\end{abstract}

%Move introduction to second page
\pagebreak
\pagenumbering{arabic}% Arabic-numbered pages (start from 1)

\section{Introduction} 

An important problem in the social sciences is estimating the effect of a policy intervention on an outcome over time. When interventions take place at an aggregate level (e.g., city or state), researchers make causal inferences by comparing the post-intervention outcomes of affected (``treated'') units against the outcomes of unaffected units (``controls'').

The synthetic control method (SCM) \citep{abadie2010synthetic} is a popular method for making causal inferences on observational time-series. The method compares a single treated unit with a synthetic control that combines the outcomes of multiple control units on the basis of their pre-intervention similarity with the treated unit. Specifically, the synthetic control is constructed by choosing a convex combination of weights $\mathbf{w} = (w_1, \ldots, w_j), w_j \geq 0, \sum w_j =1$, of control time-series that minimizes $\|\mathbf{X_1} - \mathbf{X_0} \mathbf{w}\|_\mathbf{v}$, where $\mathbf{X_1}$ and $\mathbf{X_0}$ are the pre-intervention covariates of the treated and control units, respectively, and $\mathbf{v}= (v_1, \ldots, v_J)$ are importance weights chosen to minimize the prediction error produced by $\mathbf{w}^{*} \mathbf{v}$ during cross-validation. 

The SCM has several limitations. First, the convexity restriction of the synthetic control estimator precludes dynamic, nonlinear interactions between multiple control units. \citet{ferman2016revisiting} point out that the convexity restriction implies that the SCM estimator may be biased even if selection into treatment is only correlated with time-invariant unobserved covariates. Second, the specification of the estimator can produce very different results. \citet{ferman2018cherry} show how cherry-picking between common SCM specifications can facilitate $p$-hacking. Third, while SCM can be generalized to handle multiple treated units \citep[e.g.,][]{dube2015pooling,xu2017generalized}, the generalized SCM is not capable of sharing model weights when predicting the outcomes of multiple treated units. Fourth, pre-intervention covariates are not available in all empirical applications.  

This paper proposes an alternative to SCM that is capable of automatically selecting appropriate control units at each time-step, allows for nonconvex combinations of control units, and does not rely on pre-intervention covariates. The method uses recurrent neural networks (RNNs) to predict a counterfactual time-series of treated units using only control unit outcomes as model inputs. RNNs are a class of neural networks that take advantage of the sequential nature of time-series data by sharing model parameters across multiple time-steps \citep{el1995}. Non-parametric models such as RNNs are useful for prediction problems because we do not have to assume a functional form on the data. In addition, RNNs can learn the most useful nonconvex combination of control unit outcomes at each time-step for generating counterfactual predictions. Relaxing the convexity restriction is useful when the data-generating process underlying the outcome of interest depends nonlinearly on the history of its inputs. 

RNNs are end-to-end trainable and are very flexible to a given sequential prediction problem. For example, they are capable of sharing learned parameters across time-steps and multiple treated units. Regularization methods such as dropout can easily be used to prevent overfitting during the training process, which is problematic when the networks learn an overreliance on a few
model inputs. Moreover, an attention mechanism can be included in the model in order to discern the contribution of each model input to the predicted counterfactual. 

The proposed method builds on a new literature that uses machine learning techniques such as matrix completion \citep{athey2017matrix} and the Lasso \citep{doudchenko2016balancing} for data-driven alternatives to SCM. RNNs are a natural choice for generating counterfactual predictions because they are specifically structured for sequential data and have been shown to outperform various linear models on time-series prediction tasks \citep{cinar2017position}. In a series of placebo tests using data common to the SCM literature, I present evidence that the RNN-based method outperforms SCM in terms of minimizing prediction error while maintaining a comparable false positive rate. 

In the section immediately below, I describe the approach of using RNNs for counterfactual time-series prediction; Section \ref{eval} details the procedure for evaluating the models in terms of predictive accuracy and statistical significance; Section \ref{placebo} presents the results of the placebo tests and discusses when the proposed method is expected to outperform SCM; Section \ref{conclusion} concludes by discussing the contributions of the paper and offering potential avenues for future research. 

%\setcounter{section}{1}
\section{Counterfactual prediction} \label{prediction}

The proposed method estimates the causal effect of a discrete intervention in observational time-series data; i.e., settings in which treatment is not randomly assigned and there exists both pre- and post-intervention period observations of the outcome of interest. \citet{brodersen2015inferring} originally propose an alternative to SCM that uses the pre-period time-series of control units to train a model to predict the counterfactual time-series of the treated unit. The key assumption of this approach is that the relationship between predictors $\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(\tau)}$ and the treated time-series $\mathbf{y}^{(1)}, \ldots, \mathbf{y}^{(\tau)}$ modeled prior to the intervention persists after the intervention. Similar to other methods of causal inference, the approach also assumes that the control units are not themselves affected by the intervention; i.e., there is no spillover effect that would indeterminately bias the estimate and possibly lead to a false positive. 

In its most basic form, the pre-period relationship can be modeled by linear regression, 

\begin{equation}
  \mathbf{y}^{(t)} =  \alpha_0 + \beta \mathbf{x}^{(t)} + \epsilon^{(t)} \hspace{10mm} \forall \, t = 1, \ldots, n, 
\end{equation}
\noindent
where time-step $t = 1, \ldots, n, \ldots, \tau$ is the temporal ordering of the time-series and $n$ denotes the end of the pre-period. As long as $\mathbf{x}^{(t)}$ was not impacted by the intervention, it is plausible that the modeled relationship persists after the intervention. The fitted model is then used to predict the counterfactual time-series of the treated group in the post-period: 

\begin{equation}
  \mathbf{\hat{y}}^{(t)} =  \alpha_1 + \beta \mathbf{x}^{(t)} + \zeta^{(t)}  \hspace{10mm} \forall \, t = n+1. \ldots, \tau, 
\end{equation}

The key assumption of this model is that the relationship between controls $\mathbf{x}^{(t)}$ and the treated time-series $\mathbf{y}^{(t)}$ modeled prior to the intervention persists after the intervention. Under this assumption, the inferred causal effect of the intervention on the treated group is the difference between the observed time-series of the treated units and the counterfactual time-series that would have been observed in the absence of the intervention:

\begin{equation}
  \mathbf{\hat{\phi}}^{(t)} = \mathbf{y}^{(t)} - \mathbf{\hat{y}}^{(t)} \hspace{10mm} \forall \, t = n+1, \ldots, \tau. \label{eq:pointwise}
\end{equation}

This treatment effect is calculated at every post-period time-step and is thus useful for understanding the temporal evolution of the causal effect.

\subsection{Relationship to covariate shift and domain adaptation}

%1. Train on X_c to predict Y_c
%2. Fit model weights on X_t to predict Y_t

%(Use both outcomes or observational controls as X_c, X_t) 

%*Relate to covariate shift/domain adaptation
%Domain adaptation - assume distribution of X_c similar to X_t (might be violated in the application)
%in the context of domain adaptation and covariate shift, where training and test data are drawn from different distributions 
%huang2007correcting,cortes2008sample,cortes2010learning,bickel2009discriminative

%*cortes2008sample - importance weighting of loss fn to address the problem of covariate shift.
%\citet{athey2017matrix} note that their estimator penalizes the errors for each observed value equally without regard to the fact that the probability of missingness (i.e., the propensity score), $\Pr (M_{it} = 1 | \mathbf{Y_{\mathrm{obs}}}, \phi)$, increases with $t$. The authors suggest weighting the loss function by the propensity score, which is similar to the importance weighting scheme proposed by \citet{cortes2008sample} to address the problem of covariate shift.

\section{RNNs for counteractual prediction}

Unlike linear models, RNNs are non-parametric and are structured for time-series data. RNNs consist of a hidden state $\mathbf{h}^{(t)}$ and an output $\mathbf{y}^{(t)}$ which operate on a sequence $\mathbf{x}^{(t)}$. At each time-step $t$, RNNs input $\mathbf{x}^{(t)}$ and pass it to the hidden state, which is updated with a function $g^{(t)}$ using the entire history of the sequence \citep[pp. 337][]{goodfellow2016deep}:

\begin{align}
\mathbf{h}^{(t)} &= g^{(t)} \left(\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(t)} \right) \nonumber \\ 
&= f \left( \mathbf{h}^{(t-1)}, \mathbf{x}^{(t)} \right), \label{eq:hidden}
\end{align} where $f(\cdot)$ is a nonlinear function that operates on all time-steps and input lengths. The updated hidden layer is used to generate a sequence of output values $\mathbf{o}^{(t)}$ in the form of log probabilities that correspond to $\mathbf{x}^{(t)}$. The loss function computes $\mathbf{\hat{y}}^{(t)} = \mathrm{linear} (\mathbf{o}^{(t)})$ and compares this value to $\mathbf{y}^{(t)}$.

\subsection{Encoder-decoder networks}

A special variant of RNNs that are suitable for handling variable-length sequential data are encoder-decoder networks \citep{cho2014learning}. Encoder-decoder networks are the standard for neural machine translation \citep{bahdanau2014neural,vinyals2014grammar} and are also widely used for predictive tasks, including speech recognition \citep{chorowski2015attention} and time-series forecasting \citep{zhu2017deep}. 

Encoder-decoder networks are trained to estimate the conditional distribution of the output sequence given the past input sequence, e.g., $p (\mathbf{y}^{(1)}, \ldots, \mathbf{y}^{(t)} | \mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(t)})$, where the input and output sequence lengths can differ. The encoder RNN reads in $\mathbf{x}^{(t)}$ sequentially and the hidden state of the network updates according to Eq. \ref{eq:hidden}. The hidden state of the encoder is a context vector $\mathbf{c}$ that summarizes the input sequence, which is copied over to the decoder RNN. The decoder generates a variable-length output sequence by predicting $\mathbf{y}^{(t)}$ given the encoder hidden state and the previous element of the output sequence. Thus, the hidden state of the decoder is updated recursively by

\begin{equation}
\mathbf{h}^{(t)} = f \left( \mathbf{h}^{(t-1)}, \mathbf{y}^{(t-1)}, \mathbf{c} \right), \label{eq:decoder}
\end{equation} and the conditional probability of the next element of the sequence is 

\begin{equation}
P (\mathbf{y}^{(t)} | \mathbf{y}^{(1)}, \ldots, \mathbf{y}^{(t-1)}, \mathbf{c}) =  f \left( \mathbf{h}^{(t)}, \mathbf{y}^{(t-1)}, \mathbf{c} \right).
\end{equation}  Effectively, the decoder learns to generate outputs $\mathbf{y}^{(t)}$ given the previous outputs, conditioned on the input sequence. 

\subsection{Variational autoencoder}

The encoder-decoder model described above predicts the next element of a sequence conditioned on the previous element and an evolving hidden state. While this architecture is effective for many sequence-based predictive tasks, the model does not learn a vector representation of the full sequence. The variational autoencoder (VAE) \citep{kingma2013auto} is a generative model that learns a latent variable model for input sequences such that new sequences can be generated from sampling from the latent space. Similar to encoder-decoder networks, the VAE has an encoder that learns a latent representation of the input sequence and a decoder that maps the representation back to the inputs. The VAE architecture differs from encoder-decoder networks in that the VAE doesn't have a final dense layer that compares the decoder outputs to output sequences $\mathbf{y}^{(t)}$ (i.e., it is a ``self-supervised'' technique).\footnote{Fig. SM-\ref{encoder-decoder} and Fig. SM-\ref{vae} illustrates the architectures of encoder-decoder networks and the VAE, respectively.} The other difference is that the VAE maps the inputs to a distribution over latent variables. 

The RNN-based VAE proposed by several researchers \citep{fabius2014variational, chung2015recurrent,bowman2015generating} for sequence modeling consists of an encoder RNN that maps $\mathbf{x}^{(t)}$ to a distribution over parameters of a latent space $q$. The model then randomly samples $\mathbf{z}$ from the latent distribution, $q(\mathbf{z} | \mathbf{x}^{(t)}) = q (\mathbf{z}; f(\mathbf{x}^{(t)}; \theta))$, where $f(\cdot)$ is a differentiable function with respect to parameter $\theta$ \citep[pp. 699][]{goodfellow2016deep}.\footnote{In the empirical applications, $f(\cdot)$ take the form of a log-normal distribution.} Finally, a decoder RNN takes the form of a conditional probability model $P (\mathbf{x}^{(t)} | \mathbf{z})$. The parameters of the model are learned by maximizing the loss function, which takes the difference between the log-likelihood between the decoder outputs and $\mathbf{x}^{(t)}$ and the relative entropy between  $q(\mathbf{z} | \mathbf{x}^{(t)})$ and the model prior $p (\mathbf{z})$. The latter component of the loss function acts as regularizer by forcing the learned latent distribution to be similar to the model prior. 

\section{Predictive accuracy and statistical significance} \label{eval}

In applications in which the counterfactual time-series is known (i.e., placebo tests) models can be evaluated in terms of the MSPE between the predicted and actual post-intervention time-series among control units. Specifically, I calculate:

\begin{equation}
	\text{MSPE} = \frac{1}{\tau-n} \sum_{n+1: \tau}^{\tau} \left(\hat{\phi}^{(t)} \right)^2, \label{mspe}
\end{equation} where $\hat{\phi}^{(t)}$ is defined in Eq.~\ref{eq:pointwise}. Eq. \ref{mspe} measures the accuracy of the counterfactual predictions, and consequently the accuracy of the estimated treatment effect. However, this metric does not tell us anything about the statistical significance of estimated treatment effects. 

\citet{abadie2010synthetic} propose a randomization inference approach for calculating the exact distribution of placebo effects under the null hypothesis. Following \citet{cavallo2013catastrophic}, the procedure described below extends this method to the case of multiple (placebo) treated units by constructing a distribution of \emph{average} placebo effects under the null hypothesis:

\begin{enumerate} 
	\item Estimate the observed test static $\mu^{*}$ by estimating Eq. \ref{mspe} for all $J$, which results in a matrix of dimension $(\tau-n) \times J$. Taking the row-wise mean results in a $\tau-n$-length array of observed average placebo treated effects. 
	\item Calculate every possible average placebo effect $\mu$ by randomly sampling without replacement which $J-1$ control units are assumed to be treated. There are $\mathcal{Q} = \sum\limits_{g=1}^{J-1} {J \choose g}$ possible average placebo effects. The result is a matrix of dimension $(\tau-n) \times \mathcal{Q}$. Note that $\mathcal{Q}$ can be computationally burdensome when there are many control units. In the applications described below, I set $\mathcal{Q} = 10,000$ in which $J > 16$.
	\item Take a column-wise sum of the number of $\mu$ that are greater than or equal to $\mu^{*}$.  
\end{enumerate}

Each element of the $(\tau-n) \times J$ matrix of counts obtained from the last step is divided by $\mathcal{Q}$ to estimate an array of exact two-sided $p$ values, $\hat{p}$. I then calculate a single false positive rate by $\text{FPR} = \frac{\text{FP}}{(\tau-n) \times J}$, where $\text{FP}$ is the number of false positives defined as the number of $p$-values less than or equal to $\alpha=0.5$ and $J$ is the number of placebo treated units.

\subsection{Randomization confidence intervals}

Assuming that treatment has a constant additive effect $\Delta$, I construct an interval estimate for $\Delta$ by inverting the randomization test. Let $\delta_\Delta$ be the test statistic calculated by subtracting all possible $\mu$ by $\Delta$. I derive a two-sided randomization confidence interval by collecting all values of $\delta_\Delta$ that yield $\hat{p}$ values greater than or equal to a significance level $\alpha$. I find the endpoints of the confidence interval by randomly sampling 1,000 values of $\Delta$.

\section{SCM placebo tests} \label{placebo}

I evaluate the proposed RNN-based approach on three datasets common to the SCM literature. In each dataset, I remove the actual treated unit and evaluate the models on their ability to produce low error rates on control units; i.e., estimating treatment effects of zero. A secondary evaluation criteria is the probability of falsely rejecting the null hypothesis of the randomization test as measured the FPR. The synthetic control estimator is implemented using the publicly available \textsf{R} code associated with each of the three referenced studies, and importance weights are chosen by cross-validation. 

\subsection{RNNs implementation details}

In the following application, I train a baseline RNN in the form of a single unidirectional Long Short-Term Memory (LSTM) network \citep{schmidhuber1997long} with output space dimensionality equivalent to the number of treated units. The encoder takes the form of a two-layer bidirectional LSTMs, each with 128 hidden units, and the decoder is a single-layer Gated Recurrent Unit (GRU) \citep{chung2014} also with 128 hidden units. An attention mechanism in the form of a softmax mask is included before the first hidden layer in order to generate a normalized distribution of the importance of each time-step regarding an input. In contrast to attention mechanisms used in neural machine translation to assist networks in learning the correct alignment between image pixels and target characters \citep{cho2014learning,2017arXiv171204046P}, the attention mechanism used in this paper is not expected to help the model perform better, but to help understand which time-steps and inputs contribute to the prediction.

RNN weights are learned with stochastic gradient descent on mean squared prediction error (MSPE), $L^{(t)}_{\text{MSPE}} = \E \left[\left(\mathbf{y}^{(t)} - \mathbf{\hat{y}}^{(t)} \right)^2 \right]$ using \texttt{Adam} stochastic optimization \citep{kingma2014adam}. As a regularization strategy, I apply dropout to the inputs and L2 regularization losses to the network weights. The networks are implemented with the \texttt{Keras} neural network library \citep{chollet2015keras} in Python on top of a TensorFlow backend. RNNs are trained in batches of size four or eight for 5,000 to 10,000 epochs, which takes about 20 minutes to run on a 12GB NVIDIA Titan Xp GPU.

\subsection{Basque Country} 

\citet{abadie2003economic} estimate the economic impact of terrorism in the Basque Country during the period of 1968 to 1997 by comparing per-capita gross domestic product (GDP) in the Basque Country against a synthetic control region without terrorism. The synthetic control is constructed using pre-period measures of illiteracy, educational attainment, investment, and means of GDP. The time series begins in 1955, which leaves only $n = 14$ pre-period time-steps. 

Figs. SM-\ref{basque-plot-effects} plots estimated treatment effects on control units for each model. We observe considerable variability in the SCM estimates and not as much variability in the RNNs. This is explained by the fact that SCM cannot handle multiple (placebo) treated units and thus a separate model has to be run for each (placebo) treated unit. RNNs can handle multiple treated units and thus benefit from parameter sharing across treated units. The standard deviation from the mean MPSE, which is reported in the first column of Panel A of Table \ref{fig:benchmark-table}, indicates that SCM is comparatively more variable in terms of its predictive accuracy. The baseline LSTM yields the lowest mean MSPE, $0.007 \pm 0.002$. 

\begin{table}[htbp]
\begin{center}
\caption{Evaluation metrics on SCM placebo tests.\label{fig:benchmark-table}}
\resizebox{\width}{!}{\input{/media/jason/Dropbox/github/rnns-causal/paper/rnns-results}}
\resizebox{\width}{!}{\input{/media/jason/Dropbox/github/rnns-causal/paper/rnns-fpr}}\\
\footnotesize{{NOTE: Error bars represent $\pm$ one standard deviation from the MSPE.}}
\end{center}
\end{table}

Fig. SM-\ref{basque-plot-pvalues} plots the per-period randomization $p$-values corresponding to treatment effects on treated and control units. $p$-values corresponding to treatment effects on the actual treated unit (i.e., Basque Country) are made by comparing the per-period treatment effects on Basque Country against the null distribution of average placebo effects. SCM has the comparatively lowest FPR (Panel B of Table \ref{fig:benchmark-table}) among all models: the model falsely rejects the null hypothesis about a quarter of the time. Note that the reported $p$-values are not adjusted for multiple comparisons.

\subsection{California} 

\citet{abadie2010synthetic} applies SCM to estimate the effects of a large-scale tobacco control program implemented in California in 1988. The study spans the period of 1970 to 2000, providing $n=19$ pre-period time-steps.  The synthetic control is constructed using pre-period covariates including income, beer sales, demographics, and means of the dependent variable, which is log per-capita cigarette consumption. 

Fig. SM-\ref{california-plot-effects} shows that for RNNs, control unit treatment effects are tightly centered around zero --- as expected --- whereas SCM control treatment effects (Fig. SM-\ref{california-plot-effects-synth}) are more dispersed. Encoder-decoder networks yield the lowest mean MSPE, $0.005 \pm 0.004$, while yielding comparatively higher FPR.  

\subsection{West Germany} 

Lastly, \citet{abadie2015comparative} constructs a synthetic West Germany in order to estimate the impact of the 1990 German reunification on log real per-capita GDP during a post-period that extends to 2003. The time-series begins in 1960, which leaves $n=30$ pre-period time-steps. The synthetic control is constructed using pre-period means of GDP, trade, industry, schooling, and investment. 

SCM and RNN-based approaches both assume the absence of spillover effects. \citet{abadie2015comparative} acknowledge that spillover effects is a valid concern in their study because German reunification likely have effects on GDP in the 16 OECD member countries that serve as controls. Indeed, Fig. SM-\ref{germany-plot-effects} shows that RNNs estimate mostly positive (and increasing) treatment effects on control units, which suggests that reunification might have had a less negative impact on German GDP than the authors' estimates suggest.

Overall, the baseline LSTM yields the lowest error in terms of mean MSPE, $0.02 \pm 0.01$, with a FRP comparable to SCM.

\subsection{Discussion}

I compare the predictive accuracy of the RNN-based approach against SCM by running a series of placebo tests using data from three datasets common to the SCM literature. The models are evaluated primarily on their ability to produce low error rates on control units (i.e., estimating treatment effects of zero); a secondary evaluation criteria is minimizing the probability of falsely rejecting the null hypothesis of the randomization test. 

I find that either encoder-decoder networks or LSTM outperform SCM on each of the three datasets in terms of having the lowest MSPE, with FPRs comparable to SCM. The baseline LSTM outperforms encoder-decoder networks in two of the three datasets. When applied to datasets with low-dimensional predictor sets, the LSTM performs well but deeper networks such as encoder-decoder networks are susceptible to overfitting. Overfitting in this case means that the networks learn dependencies on a small subset of predictors and cannot generalize well to unseen data. Overfitting occurs when training encoder-decoder networks on the Basque Country dataset (Fig. SM-\ref{encoder-decoder-loss-basque-treated}), which is has the lowest dimensions of the three SCM datasets. Even in this case of obvious overfitting, model check-pointing is employed so that the model with the lowest validation error is used to produce counterfactual time-series.

The results suggest that RNNs should outperform SCM in all cases as long as the complexity of the network architecture is proportional to the dimension of the predictor set. Encoder-decoder networks outperform the other models when the predictor set is comparatively large (i.e., $J=38$ in the California dataset), while the baseline LSTM outperforms all other models on smaller predictor sets ($J=16$ for Basque Country and West Germany datasets). 

\section{Application: Homestead acts and state capacity} \label{state-capacity}

%motivate: Dube - education/nation-building
In this section, I estimate the causal impacts of homestead policies on state government education spending. I create a state-level measure of state government education spending from the records of 48 state governments during the period of 1783 to 1932 \citep{sylla1993sources} and the records of 16 state governments during the period of 1933 to 1937 \citep{sylla1995sourcesa,sylla1995sourcesb}. Comparable measures for 48 states are drawn from U.S. Census special reports for the years 1902, 1913, 1932, 1942, 1962, 1972, and 1982 \citep{haines2010}.

The data pre-processing steps are as follows. The measure is inflation-adjusted according to the U.S. Consumer Price Index \citep{williamson2017seven} and scaled by the total free population in the decennial census \citep{haines2010}. Missing values are imputed separately in the pre- and -post-periods by carrying the last observation forward and remaining missing values are imputed by carrying the next observation backward. The raw outcomes data are log-transformed to alleviate exponential effects. Lastly, I remove states with no variance in the pre-period outcomes, resulting in complete $\text{N} \times \text{T}$ matrices of size $33 \times 159$ and $34 \times 158$ for the expenditures and revenues outcomes, respectively. 

In this application, public land states are the treated units and state land states --- i.e., states that were not crafted from the public domain and were therefore not directly affected by homestead policies --- serve as control units. This group includes states of the original 13 colonies, Maine, Tennessee, Texas, Vermont, and West Virginia. Aggregating to the state level approximately 1.46 million individual land patent records authorized under the HSA,\footnote{Land patent records provide information on the initial transfer of land titles from the federal government and are made accessible online by the U.S. General Land Office (\url{https://glorecords.blm.gov}).} I determine that the earliest homestead entries occurred in 1869 in about half of the western frontier states, about seven years following the enactment of the HSA.

I train a encoder-decoder network to predict the counterfactual time-series of public land states, using only their previous history to generate predictions. Similar to the placebo tests on the SCM datasets, I evaluate the models two ways: first, I monitor the loss over 2,000 training epochs and save the model weights with the lowest error on a validation set consisting of the final 10\% of the time-series.\footnote{The models use both dropout and L2 regularization to control for overfitting on the training set. Figs. SM-\ref{encoder-decoder-loss-capacity-south} and SM-\ref{encoder-decoder-loss-capacity-west} record the training history of each model.} Second, I calculate MSPE (Eq.~\ref{mspe}) on $J=18$ state land states that serve as placebo treated units. Encoder-decoder networks outperform the baseline LSTM in terms of minimizing the MSPE in placebo tests (Tables SM-\ref{encoder-decoder-mpse} and SM-\ref{lstm-mpse}).

Fig.~\ref{state-capacity-plot} plots the observed and counterfactual time-series for each outcome and region. Counterfactual predictions of state government finances in the absence of homestead acts generally tracks the observed time-series until the turn of the century, at which the counterfactual flattens and diverges from the increasing observed time-series. This delay can potentially be explained by the facts that homesteaders were required to make improvements on land for five years before filing a grant and homestead entries did not substantially accumulate until after the 1889 cash-entry restriction (Figs. SM-\ref{homestead-map} and SM-\ref{acres-time}). 

Taking the difference between the observed and predicted time-series (Eq.~\ref{eq:pointwise}) yields time-specific estimates of treatment effects. Fig.~\ref{state-capacity-plot-effects} plots the temporal evolution of treatment effect estimates over the entire post-period and 95\% randomization confidence intervals that are constructed by inverting the randomization test described in the previous section.\footnote{Fig. SM-\ref{state-capacity-plot-pvalues} plots the time-specific estimates of randomization $p$-values inferred from the exact distribution of average placebo effects under the null hypothesis.} Fifty years after its passage, the estimated impact of the HSA on western state government education spending and revenue is 0.005 log points [-0.16, 0.19], and 0.61 log points [-0.19, 1.53], respectively. The confidence intervals surrounding these time-specific estimates contain zero, which implies that the estimated impacts are not significantly more extreme than the placebo treated effects estimated at the same time-step. The confidence interval on the estimated impact of the HSA on western state government expenditure in 1912, an increase of 0.17 log points [0.004, 0.3], does not contains zero, which implies that the estimated impact is significantly more extreme than the placebo treated effects. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{/media/jason/Dropbox/github/land-reform/unsourced-plots/mc-educ-pc.png}
	\caption{MC-NNM estimates of treatment exposure on state government education spending, 1809 to 1982.\label{mc-educ-pc}} 
\end{figure}

An important characteristic of the estimates plotted in Fig.~\ref{state-capacity-plot-effects} is the progressive widening of confidence intervals. Intuitively, counterfactual predictions become more uncertain as we move farther into the future. In the present application, confidence intervals may become implausibly wide because the post-period extends well into the twentieth century. In order to compare with the DD estimates described in the section below, I average over the entire post-period and find no evidence that the HSA impacted western state government education spending, 0.07 [-0.32, 0.46], expenditure, 0.16 [-0.21, 0.57], or revenue, 0.05 [-0.33, 0.46]. Estimates on the impact of the SHA on state capacity in the South are in the same direction and similar magnitudes. 

The attention mechanism embedded within the networks provides clues as to which predictors the networks favor at each time-step during the training process. Fig. 3 plots the attention mechanism applied to the test data, where each cell of the heatmap represents the mean attention weight regarding each time-step and predictor across all test samples. The attention distribution suggests that the networks favor heavily winner margins from the 1975 election and also winner margins from the 1988 and 2003 elections when predicting the post-period. Attention is generally distributed evenly across predictors, which suggests that the networks rely on many different non-treated cities to construct the counterfactual.

\begin{figure}[htbp]
	\centering
%	\includegraphics[width=\textwidth]{/media/jason/Dropbox/github/land-reform/unsourced-plots/mc-educ-pc.png}
	\caption{Attention mechanism as a function of predictors (i.e., winner margins by city) and time-steps for encoder-decoder
		networks. Attention is the normalized (softmax) distribution of the importance of each time-step regarding a predictor.\label{attn-plot}} 
\end{figure}

\section{Conclusion} \label{conclusion}

This paper proposes a novel alternative to SCM, which is growing in popularity in the social sciences despite its limitations; the most obvious being that the choice of specification can lead to different results, and thus facilitate $p$-hacking. By inputting only control unit outcomes and not relying on pre-period covariates, the proposed method offers a more principled approach than SCM. RNNs are also capable of handling multiple treated units and can learn nonconvex combinations of control units. The former attribute is useful because the model can share parameters across treated units, and thus generate more precise predictions in settings in which treated units share similar data-generating processes. The latter attribute is beneficial when the data-generating process underlying the outcome of interest depends nonlinearly on the history of its inputs.

The RNN-based approach joins a new generation of data-driven machine learning techniques such as matrix completion \citep{athey2017matrix} and the Lasso \citep{doudchenko2016balancing} for generating counterfactual predictions. While the strength of SCM lies in its simplicity in setup and implementation, several problems arise from the lack of guidance on how to specify the SCM estimator. \citet{kaul2015synthetic} show, for instance, that the common practice of including lagged versions of the outcome variable as separate predictors can render all other covariates irrelevant.% In addition, the common practice of using cross-validation to select importance weights can yield multiple values and consequently different results \citep{klossner2017comparative}. 

Machine learning techniques in general have an advantage over SCM in that they automatically choose appropriate predictors without relying on pretreatment covariates; this capability limits ``researcher degrees of freedom'' that arises from choices on how to specify the model. RNNs have an advantage over alternative machine learning algorithms because they are specifically structured to exploit the sequential nature of time-series data by sharing model parameters across time-steps.

Future research might investigate through simulations how the interaction between RNN complexity (as determined by the number of hidden layers or nodes) and data dimensionality impacts predictive accuracy. Simulations will also allow us to assess the exact impact of data dimensionality, the proportion of treated units, convexity versus non-convexity in the modeled relationship, and the length of the pre-period on the choice between RNNs and SCM.

%This paper makes a methodological contribution in proposing a novel alternative to SCM for estimating the effect of a policy intervention on an outcome over time in settings where appropriate control units are unavailable. In placebo tests, RNN-based models outperform SCM in terms of predictive accuracy while yielding a comparable proportion of false positives. RNNs have advantages over SCM in that they are structured for sequential data and can learn nonconvex combinations of predictors, which is useful when the data-generating process underlying the outcome depends nonlinearly on the history of predictors.% Future work might formalize a set of assumptions necessary for the approach to be valid and provide further proof of consistency of the estimator under these assumptions.


\newpage

\bibliographystyle{rss}
\begin{singlespace}
	\begin{footnotesize}
		\begin{multicols}{2}
			\bibliography{references}
		\end{multicols}
	\end{footnotesize}
\end{singlespace}

\itemize
\end{document}